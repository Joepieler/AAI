from tensorflow.python.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras import backend

import numpy as np
import cv2
import random



"""
Requirements to run the AI
If file X.pickle and Y.pickle exits then you don't need all the covers of the books. The images are saved in X.pickle and the training results are saved in Y.pickle

If those files don't exist then the AI needs a folder with the covers and file with the training results named output.csv
It will create for you the X.pickle Y.pickle so it doesn't have to create the array every time you run the AI.

Also the AI will output the output values of the second to last layer so you can put it in a K Means to determine the number of clusters and the labels.


if folder covers and file output.csv do not exist then create the folder and use the script Image_downloader.py to download the images out of the file BX-Books-full.csv
for the file output.csv it can be generated by running setup.py and needs the following files: BX-Book-Ratings.csv and BX-Books-full.csv
"""

"""settings for the AI"""
Length = 121764 # The number of images in the covers folder.
IMG_SIZE = 40 # The size where in the image are scaled to.
Percentage_Training_Data = 90 # The percentage of the data that is for training.
Output_for_KMeans = False # set to True if you want the output of the second to last layer in a file

# array for the data
Training_data = []


"""to create the Data"""
def create_traing_data():
    book_ratings = np.genfromtxt("output.csv", delimiter=",", usecols=(0,1), loose=False, invalid_raise=False, dtype=str)
    bad = np.array('bad')
    good = np.array('good')

    for i in range(Length):
        try:
            #filter only good images and have a rating of bad or good
            new_image = cv2.imread("covers/" + str(i) + ".jpg")
            if new_image.all() != None:
                new_image = cv2.resize(new_image, (IMG_SIZE, IMG_SIZE))     # resize the image
                cv2.colorChange(new_image, new_image, cv2.COLOR_RGB2HSV)    # change from RGB to HSV
                h, s, v = cv2.split(new_image)                              # split the HSV in separate values
                new_image = cv2.merge(h, v)                                 # Combine HUE and saturation back to a HS image
                if np.char.equal([book_ratings[i][1]], bad):                # Bad images are added to the Training_data
                    Training_data.append([new_image, 0])
                elif np.char.equal([book_ratings[i][1]], good):             # Good images are added to the Training_data
                    Training_data.append([new_image, 1])
        except(AttributeError):                                             # if image is NULL (there are bad or not downloadable covers in the amazon list) they are skipped
            continue

"""If X.pickle and Y.pickle exist then open them and use them as Trainingsdata else create them"""
try:
    import pickle

    X = []
    Y = []
    pickle_in = open("X.pickle", "rb")
    X = pickle.load(pickle_in)
    pickle_in.close()

    pickle_in = open("Y.pickle", "rb")
    Y = pickle.load(pickle_in)
    pickle_in.close()


except FileNotFoundError:
    create_traing_data()

    '''shuffle the data so we it is random if it is a bad or good book'''
    random.shuffle(Training_data)


    for features, label in Training_data:
        X.append(features)
        Y.append(label)

    X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 2) #the images are CV2 an need to be numpy array. The 2 at the end is for amount of info per pixel (so if gray scale needs to be 1 and RGB/HSV needs to be 3

    pickle_out = open("X.pickle", "wb")
    pickle.dump(X, pickle_out)
    pickle_out.close()

    pickle_out = open("Y.pickle", "wb")
    pickle.dump(Y, pickle_out)
    pickle_out.close()

finally:
    """Info of data"""
    print("length of the data {}".format(len(X)))
    print("Number of bad books: {}. Precentage of books that are bad: : {}%".format(Y.count(0), int(Y.count(0)/ len(Y) * 100)))
    print("Number of good books: {}. Precentage of books that are good : {}%".format(Y.count(1), int(Y.count(1)/ len(Y) * 100)))


"""normelize from 0-255 to 0.0-1"""
X = X/255.0

"""split in training data end verrification data"""
Total_Length = len(X)
X_train = []
Y_train = []

X_test = []
Y_test = []

for i in range(Total_Length):
    if i < Total_Length/100*Percentage_Training_Data:
        X_train.append(X[i])
        Y_train.append(Y[i])
    else:
        X_test.append(X[i])
        Y_test.append(Y[i])


"""Sets the weights because there are more good books then bad books"""
weight_for_0 = 1.0 / Y_train.count(0)
weight_for_1 = 1.0 / Y_train.count(1)
class_weight = {0: weight_for_0, 1: weight_for_1}


"""Create the Neural Network"""
model = Sequential()

"""Input layer"""
model.add(Conv2D(128, (3, 3), input_shape=X.shape[1:]))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

"""To go from conv2d layers to dense you need to flatten them first"""
model.add(Flatten())

model.add(Dense(64))
model.add(Activation("relu"))

model.add(Dense(16))
model.add(Activation("relu"))


model.add(Dropout(0.1))

"""Output layer"""
model.add(Dense(2))
model.add(Activation("softmax"))

""""Compile the model"""
model.compile(loss="binary_crossentropy",
              optimizer="adam",
              metrics=["accuracy"])


"""from a list to np array. The NN accepts only np array"""
X_train = np.array(X_train)
Y_train = np.array(Y_train)


"""Train the model"""
model.fit(X_train, Y_train, class_weight=class_weight, batch_size=32, epochs=5, validation_split=0.1)


"""from a list to np array. The Neural Network accepts only np array"""
X_test = np.array(X_test)
Y_test = np.array(Y_test)


""""Evaluate the model"""
TestLoss, TestAccuracy = model.evaluate(X_test,  Y_test, verbose=2)
print("The loss is {} and the accuracy is {}".format(TestLoss, TestAccuracy))


"""To put the output of the second to last layer in a file This can be used for """
if Output_for_KMeans:
    get_layer_output = backend.function(inputs=model.layers[0].input, outputs=model.layers[12].output)
    layer_output = get_layer_output(X_test)

    pickle_out = open("layer_output.pickle", "wb")
    pickle.dump(layer_output, pickle_out)
    pickle_out.close()

    pickle_out = open("Y_labels", "wb")
    pickle.dump(Y_test, pickle_out)
    pickle_out.close()



